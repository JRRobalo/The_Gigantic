{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summary #","metadata":{}},{"cell_type":"markdown","source":"<div class= 'alert alert-block alert-success'>\n<b>The Titanic:</b>\n\nThe titanic sunk. Many died.\\\nCan we <b>predict those who survived</b>?\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"# Import packages, load data #","metadata":{}},{"cell_type":"code","source":"### IMPORT\n### GENERAL PACKAGES\nimport numpy as np, pandas as pd, statsmodels.api as sm\nimport os\n\n### SETUP\npd.set_option('display.max_columns', None)\n\n### PLOTTING\nimport seaborn as sns, matplotlib.pyplot as plt\n\n### DATA MANIPULATION\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, PredefinedSplit\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\n### MODELS\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import CategoricalNB, MultinomialNB, GaussianNB, ComplementNB, BernoulliNB\n\n### EXPORTING MODELS\nimport pickle\n\n### VALIDATION\nfrom sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay,classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LOOK FOR FILES\n!ls /kaggle/input/titanic/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LOAD DATA\ndf_train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"markdown","source":"# Inspect data for completeness #","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe(include='all')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class= 'alert alert-block alert-danger'>\n<b>Missing train data:</b>\n\n* 891 entries, with data types int64, float64, and string\n* many entries missing from AGE and, especially, CABIN\n</div>","metadata":{}},{"cell_type":"code","source":"# CHECKING FOR DUPLICATED VALUES\ndf_train.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CHECK TEST DATA\ndf_test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trying to figure out _why_ there is missing data","metadata":{"execution":{"iopub.status.busy":"2023-11-04T07:40:45.588868Z","iopub.execute_input":"2023-11-04T07:40:45.589298Z","iopub.status.idle":"2023-11-04T07:40:45.595960Z","shell.execute_reply.started":"2023-11-04T07:40:45.589267Z","shell.execute_reply":"2023-11-04T07:40:45.594607Z"}}},{"cell_type":"code","source":"def count_records(featcounts:str, feat:str, norecord:bool):\n    \"\"\"\n    Counts FEATCOUNTS occurrences of each category of SEX, PCLASS, EMBARKED, FAMILY=SIBSP+PARCH\n    for cases where FEAT={AGE,CABIN} records are absent or not (NORECORD={TRUE, FALSE})\n    \"\"\"\n    featcounts.append(len(df_train[(df_train[feat].isna() == norecord) & (df_train['Sex'] == 'male')]))\n    featcounts.append(len(df_train[(df_train[feat].isna() == norecord) & (df_train['Sex'] == 'female')]))\n    featcounts.append(len(df_train[(df_train[feat].isna() == norecord) & (df_train['Pclass'] == 1)]))\n    featcounts.append(len(df_train[(df_train[feat].isna() == norecord) & (df_train['Pclass'] == 2)]))\n    featcounts.append(len(df_train[(df_train[feat].isna() == norecord) & (df_train['Pclass'] == 3)]))\n    featcounts.append(len(df_train[(df_train[feat].isna() == norecord) & (df_train['Embarked'] == 'S')]))\n    featcounts.append(len(df_train[(df_train[feat].isna() == norecord) & (df_train['Embarked'] == 'C')]))\n    featcounts.append(len(df_train[(df_train[feat].isna() == norecord) & (df_train['Embarked'] == 'Q')]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# COUNTS OF NO AGE OR CABIN RECORD AT EACH PASSENGER CLASS, GENDER, PORT, FAMILY=SIBSP+PARCH\ndf_records = pd.DataFrame()\ndf_records['Feature'] = ['Gender(M)', 'Gender(F)', 'Class(1st)','Class(2nd)', 'Class(3rd)', \n                       'Port(Southampton)', 'Port(Cherbourg)', 'Port(Queenstown)']\n\ncounts_age = []; counts_noage = []; counts_cabin = []; counts_nocabin = []\n\ncount_records(counts_age, 'Age', False)\ncount_records(counts_noage, 'Age', True)\ncount_records(counts_cabin, 'Cabin', False)\ncount_records(counts_nocabin, 'Cabin', True)\n\ndf_records['Age_record'] = counts_age\ndf_records['No_age_record'] = counts_noage\ndf_records['Cabin_record'] = counts_cabin\ndf_records['No_cabin_record'] = counts_nocabin\n\nprint(\"\"\"Number of passengers with/without AGE or CABIN records depending on GENDER, ticket CLASS,\nport of EMBARKment:\\n\\n\"\"\", df_records)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Notes on data inspection:</b>\n\n* TICKET data seems useless; some are strings with only numeric values, some are alphanumeric\n* missing data could stem from port services, due to a much larger volume of inbound passengers:\n    * about 70% of missing AGE/CABIN data related to 3rd class ticket passengers, with a similar percentage for male passengers\n    * Southampton was the embarcation port for 72% of all passengers\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class= 'alert alert-block alert-warning'>\n<b>About train data completeness:</b>\n\n* 891 entries, with data types int64, float64, and string\n* 177 values missing from AGE field\n* 687 values missing from CABIN field\n* 2 values missing from EMBARK field\n* no duplicate entries\n\n<b>About test data completeness:</b>\n\n* 418 entries, with data types int64, float64, and string\n* test data also has missing values for AGE (327) and CABIN (86), and one missing entry for FARE\n* no duplicate entries\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Univariate Distributions","metadata":{}},{"cell_type":"code","source":"# CREATE LIST OF FEATURES TO KEEP\nselect_features = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ONE-DIMENSIONAL DISTRIBUTIONS\n# SET FIGURE SIZE AND GRID SIZE\nfig = plt.figure(figsize = (15,10))\ngs = plt.GridSpec(3,3, height_ratios=(1,1,1))\n\n# SET GRID POSITIONS FOR PLOTS\ng1 = plt.subplot(gs[0, 0])\ng2 = plt.subplot(gs[0, 1])\ng3 = plt.subplot(gs[0, 2])\ng4 = plt.subplot(gs[1, 0])\ng5 = plt.subplot(gs[1, 1])\ng6 = plt.subplot(gs[1, 2])\ng7 = plt.subplot(gs[2, 0])\n\n\nax1 = sns.histplot(ax=g1, data = df_train, x='Pclass',hue = 'Survived', stat='count', multiple='dodge',shrink=3)\nax1.set_xlabel('Passenger class', fontsize=12)\nax1.set_ylabel('Number of passengers', fontsize=12)\n\nax2 = sns.histplot(ax=g2, data = df_train, x='Sex',hue = 'Survived', stat='count', multiple='dodge', shrink=0.5)\nax2.set_xlabel('Passenger gender', fontsize=12)\nax2.set_ylabel('Number of passengers', fontsize=12)\n\nax3 = sns.histplot(ax=g3, data = df_train, x='Age',hue = 'Survived', stat='count', multiple='dodge',shrink=0.5)\nax3.set_xlabel('Passenger Age', fontsize=12)\nax3.set_ylabel('Number of passengers', fontsize=12)\n\nax4 = sns.histplot(ax=g4, data = df_train, x='Parch',hue = 'Survived', stat='count', multiple='dodge')\nax4.set_xlabel('Number of parents/children', fontsize=12)\nax4.set_ylabel('Number of passengers', fontsize=12)\n\nax5 = sns.histplot(ax=g5, data = df_train, x='SibSp',hue = 'Survived', stat='count', multiple='dodge')\nax5.set_xlabel('Number of siblings/spouses', fontsize=12)\nax5.set_ylabel('Number of passengers', fontsize=12)\n\nax6 = sns.histplot(ax=g6, data = df_train, x='Embarked',hue = 'Survived', stat='count', multiple='dodge', shrink=0.5)\nax6.set_xlabel('Port (Southampton/Cherbourg/Queenstown)', fontsize=12)\nax6.set_ylabel('Number of passengers', fontsize=12)\n\nax7 = sns.histplot(ax=g7, data = df_train, x='Fare',hue = 'Survived', stat='count', multiple='dodge', shrink=1)\nax7.set_xlabel('Ticket Fare', fontsize=12)\nax7.set_ylabel('Number of passengers', fontsize=12)\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"select_features.append('Sex') ; select_features.append('Pclass')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>SEX, PCLASS, EMBARKED, SIBSP, PARCH:</b>\n\n* These features seem to be promissing predictors;\n* Sex, in particular, seems to have the most predictive power.\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Creating the FAMILY features ##","metadata":{}},{"cell_type":"code","source":"# FAMILY_BIN = PARCH + SIBSP >0 OR NOT\n# DEFINE FUNCTION TO ASSIGN FAMILY VALUES BASED ON PARCH AND SIBSP\ndef fam(parch:int, sibsip:int):\n    if (parch==0) & (sibsip==0):\n        return 'no'\n    else:\n        return 'yes'\n# APPLY FUNCTION\ndf_train['Family_yn'] = df_train.apply(lambda x: fam(x.Parch, x['SibSp']), axis=1)\ndf_test['Family_yn'] = df_test.apply(lambda x: fam(x.Parch, x['SibSp']), axis=1)\n# FAMILY = PARCH + SIBSP\ndf_train['Family'] = df_train['SibSp'] + df_train['Parch']\ndf_test['Family'] = df_test['SibSp'] + df_test['Parch']\n# FAMILY SIZE, ACCORDING TO VARIATION IN SURVIVED FEATURE\ndef famsize(family):\n    if family == 0:\n        return 'alone'\n    elif family < 4:\n        return 'small'\n    else:\n        return 'large'\n# APPLY FUNCTION; pd.Categorical SETS AN ORDINALITY TO THE CATEGORIES THAT WILL BE USED WHEN, E.G., PLOTTING\ndf_train['Famsize'] = pd.Categorical(df_train['Family'].apply(famsize), categories=['alone', 'small', 'large'])\ndf_test['Famsize'] = pd.Categorical(df_test['Family'].apply(famsize), categories=['alone', 'small', 'large'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLOT FAMILY FEATURES\nfig_fam = plt.figure(figsize = (14,3))\ngs = plt.GridSpec(1,3)\n# SET GRID POSITIONS FOR PLOTS\ng1 = plt.subplot(gs[0, 0])\ng2 = plt.subplot(gs[0, 1])\ng3 = plt.subplot(gs[0, 2])\n# PLOT FAMILY, FAMILY_YN, FAMSIZE\nax_list = ['ax1', 'ax2', 'ax3']\ngpts_list = [g1, g2, g3]\nfeat_list = ['Family', 'Family_yn', 'Famsize']\nxlbl_list = ['Number of Family Members Aboard', 'Family Members Aboard: yes/no', 'Family Size (0<small<4)']\nylbl_list = ['P(Family & Survived)', '', '']\nfor axis, gp, feat, xlabel, ylabel in zip(ax_list, gpts_list, feat_list, xlbl_list, ylbl_list):\n    axis = sns.histplot(ax=gp, data = df_train, x=feat,hue = 'Survived', multiple='dodge', stat='probability')\n    axis.set_xlabel(xlabel, fontsize=12)\n    axis.set_ylabel(ylabel, fontsize=12)\nfig_fam.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# APPEND FEATURE\nselect_features.append('Famsize') ; select_features.append('Family_yn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>FAMSIZE feature:</b>\n\n* Passengers who travel without family members or with large families (more than four family members in total) are much less likely to survive;\n* Passengers travelling with up to three other family members (small families) are more likely to survive;\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Extracting Titles from the NAME feature ##","metadata":{}},{"cell_type":"code","source":"# WRITE A FUNCTION TO SPLIT THE NAME STRING AND EXTRACT THE TITLE\ndef detach(inp):\n    return list(inp.split(','))[1].split()[0]\n\n# APPLY FUNCTION TO NAME FEATURE\ndf_train['Title'] = df_train['Name'].apply(lambda x: detach(x))\ndf_test['Title'] = df_test['Name'].apply(lambda x: detach(x))\n# THERE IS A TITLE SPELLED \"THE\", WHICH BELONGS TO A COUNTESS: INDEX 759\ndf_train.loc[759,'Title'] = 'Countess'\n# GROUP TITLES IN DICTIONARY ACCORDING TO DISTRIBUTION VS SURVIVED\ntitlegroup = {}\ntitlegroup.update(dict.fromkeys(['Mr.', 'Miss.', 'Mrs.', 'Ms.'],'Common'))\ntitlegroup.update(dict.fromkeys(['Master.', 'Dr.', 'Rev.', 'Major.', 'Col.', 'Capt.'],'Professional'))\ntitlegroup.update(dict.fromkeys(['Don.', 'Sir.', 'Lady.', 'Mlle.', 'Mme.','Jonkheer.', 'Countess'],'Wealth'))\ntitledictcommon = {}\ntitledictcommon.update(dict.fromkeys(['Mr.', 'Miss.', 'Mrs.', 'Ms.'],'yes'))\ntitledictcommon.update(dict.fromkeys(['Master.', 'Dr.', 'Rev.', 'Major.', 'Col.', 'Capt.', 'Don.', 'Sir.', 'Lady.', 'Mlle.', 'Mme.','Jonkheer.', 'Countess'],'no'))\n# GROUP TITLES\ndf_train['Titlegroup'] = df_train['Title'].map(titlegroup)\ndf_test['Titlegroup'] = df_test['Title'].map(titlegroup)\n# SIMPLIFY TITLES\ndf_train['Titlecommon'] = df_train['Title'].map(titledictcommon)\ndf_test['Titlecommon'] = df_test['Title'].map(titledictcommon)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLOT TITLE FEATURES\nfig_ttl = plt.figure(figsize = (14,3))\ngs = plt.GridSpec(1,3)\n# SET GRID POSITIONS FOR PLOTS\ng1 = plt.subplot(gs[0, 0])\ng2 = plt.subplot(gs[0, 1])\ng3 = plt.subplot(gs[0, 2])\n# PLOT TITLE, TITLEGROUP, TITLESIMPLE\nax_list = ['ax1', 'ax2', 'ax3']\ngpts_list = [g1, g2, g3]\nfeat_list = ['Title', 'Titlegroup', 'Titlecommon']\nxlbl_list = ['Title', 'Title, grouped', 'Title = Common']\nylbl_list = ['P(Title & Survived)', 'P(Title & Survived)', 'P(Title & Survived)']\nfor axis, gp, feat, xlabel, ylabel in zip(ax_list, gpts_list, feat_list, xlbl_list, ylbl_list):\n    axis = sns.histplot(ax=gp, data = df_train, x=feat,hue = 'Survived', multiple='dodge', stat='probability')\n    axis.set_xlabel(xlabel, fontsize=12)\n    axis.set_ylabel(ylabel, fontsize=12)\n    axis.tick_params(axis='x', rotation = 45)\n    fig_ttl.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# APPEND FEATURE\nselect_features.append('Titlecommon')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>TITLESIMPLE feature:</b>\n\n* Passengers with TITLECOMMON=yes are less likely to survive;\n* Passengers with TITLECOMMON=no are as likely to survive as not;\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Filling the FARE field in test data ##","metadata":{}},{"cell_type":"code","source":"# PASSENGERS WITHOUT FARE DATA IN TEST DATASET\ndf_test[df_test['Fare'].isna() == True]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CALCULATE MEDIAN OF FARE FOR RELEVANT PASSENGER FEATURES IN THE TRAIN DATASET AND ASSIGN IT\n# 3rd PCLASS, MALE, NO FAMILY, SOUTHAMPTON\nmed_fare = df_train[(df_train['Pclass'] == 3) & (df_train['Sex'] == 'male') & (df_train['Family_yn'] == 'no') & (df_train['Embarked'] == 'S')]['Fare'].median()\ndf_test['Fare'] = df_test['Fare'].fillna(med_fare)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Binning FARE into Quartiles ##","metadata":{}},{"cell_type":"code","source":"# BINNING FARE VALUES INTO QUARTILES\ndef quarts(fare):\n    q1 = df_train['Fare'].quantile(0.25)\n    q2 = df_train['Fare'].quantile(0.50)\n    q3 = df_train['Fare'].quantile(0.75)\n    if fare < q1:\n        return 1\n    elif fare < q2:\n        return 2\n    elif fare < q3:\n        return 3\n    elif fare >= q3:\n        return 4\n\ndf_train['Fare_Q'] = df_train.apply(lambda x: quarts(x['Fare']), axis=1)\ndf_test['Fare_Q'] = df_test.apply(lambda x: quarts(x['Fare']), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLOT FARE FEATURES\nfig_fare = plt.figure(figsize = (14,3))\ngs = plt.GridSpec(1,3)\n# SET GRID POSITIONS FOR PLOTS\ng1 = plt.subplot(gs[0, 0])\ng2 = plt.subplot(gs[0, 1])\n# PLOT FARE, FARE_Q, FARE_Q4\nax_list = ['ax1', 'ax2']\ngpts_list = [g1, g2]\nfeat_list = ['Fare', 'Fare_Q']\nxlbl_list = ['Fare', 'Fare Quartiles']\nylbl_list = ['P(Fare & Survived)', '']\nfor axis, gp, feat, xlabel, ylabel in zip(ax_list, gpts_list, feat_list, xlbl_list, ylbl_list):\n    axis = sns.histplot(ax=gp, data = df_train, x=feat,hue = 'Survived', multiple='dodge', stat='probability')\n    axis.set_xlabel(xlabel, fontsize=12)\n    axis.set_ylabel(ylabel, fontsize=12)\nfig_fare.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# APPEND FEATURE\nselect_features.append('Fare_Q')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>FARE_Q4 feature:</b>\n\n* Passengers with FARE_Q4=no are less likely to survive;\n* Passengers with FARE_Q4=yes are more likely to survive;\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Filling the EMBARKED field ##","metadata":{}},{"cell_type":"code","source":"# THE TWO MISSING EMBARKED VALUES IN THE TRAIN DATA ARE FOR FIRST CLASS FEMALES WITHOUT FAMILY ABOARD\n# SHARING A CABIN, SO THE PORT COULD BE THE SAME\ndf_train[df_train['Embarked'].isna() == True]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LOOKING AT RECORDS FROM FEMALES, WITHOUT FAMILY, FIRST CLASS\nprint(df_train[(df_train['Pclass'] == 1) & (df_train['Family_yn'] == 'no') & (df_train['Sex'] == 'female')].groupby('Embarked')['PassengerId'].count())\n# OVER 50% PROBABILITY THAT BOTH CAME FROM CHERBOURG, SO WE'LL ASSIGN THAT AS EMBARKED PORT\ndf_train['Embarked'].fillna('C', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GROUPING EMBARKED INTO SOUTHAMPTON=Yes/No, ACCORDING TO DISTRIBUTION OF EMBARKED AND SURVIVED\ndef portsouth(port):\n    if port == 'S':\n        return 'yes'\n    else:\n        return 'no'\ndf_train['Port_SH'] = pd.Categorical(df_train['Embarked'].apply(portsouth), categories = ['no', 'yes'])\ndf_test['Port_SH'] = pd.Categorical(df_test['Embarked'].apply(portsouth), categories = ['no', 'yes'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLOT EMBARKED FEATURES\nfig_port = plt.figure(figsize = (10,3))\ngs = plt.GridSpec(1,2)\n# SET GRID POSITIONS FOR PLOTS\ng1 = plt.subplot(gs[0, 0])\ng2 = plt.subplot(gs[0, 1])\n# PLOT EMBARKED, PORT_SH\nax_list = ['ax1', 'ax2']\ngpts_list = [g1, g2]\nfeat_list = ['Embarked', 'Port_SH']\nxlbl_list = ['Port of Embarkment', 'Port = Southampton']\nylbl_list = ['P(Port & Survived)', '']\nfor axis, gp, feat, xlabel, ylabel in zip(ax_list, gpts_list, feat_list, xlbl_list, ylbl_list):\n    axis = sns.histplot(ax=gp, data = df_train, x=feat,hue = 'Survived', multiple='dodge', stat='probability')\n    axis.set_xlabel(xlabel, fontsize=12)\n    axis.set_ylabel(ylabel, fontsize=12)\nfig_port.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# APPEND FEATURE\nselect_features.append('Embarked')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>PORT_SH feature:</b>\n\n* Passengers embarking in Southampton are less likely to survive;\n* Passengers embarking in other ports are as likely to survive as not;\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Creating the CabinId feature ##","metadata":{"execution":{"iopub.status.busy":"2023-10-26T09:26:19.116613Z","iopub.execute_input":"2023-10-26T09:26:19.117048Z","iopub.status.idle":"2023-10-26T09:26:19.122787Z","shell.execute_reply.started":"2023-10-26T09:26:19.117017Z","shell.execute_reply":"2023-10-26T09:26:19.121232Z"}}},{"cell_type":"code","source":"# COUNTS OF CABIN IDENTIFIERS INCLUDING ONLY DIGITS (NOT LETTERS)\ndf_train['Cabin'].str.isnumeric().value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GET CABIN IDENTIFIER LETTER(S)\n# THERE ARE TOO MANY CABINID CATEGORIES (EX: B, BB, BBB, BBBB), SO WE'LL KEEP ONLY THE FIRST LETTER\n# THIS MEANS, FROM 1ST TO 3RD CLASS: T, A, B, C, D, E, F, G\ndef firstlet(cabin:str):\n    if pd.isna(cabin)==False:\n        return str(cabin)[0]\n# APPLY FUNCITON\ndf_train['CabinId'] = pd.Categorical(df_train['Cabin'].apply(firstlet), categories = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T'])\ndf_test['CabinId'] = pd.Categorical(df_test['Cabin'].apply(firstlet), categories = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.groupby(['Pclass', 'CabinId']).PassengerId.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\">\nENTRY 339 WAS PERMANENTLY DELETED!\n</div>","metadata":{}},{"cell_type":"code","source":"# CBN_COMP=T IS A SINGLE ENTRY FOR THE CABIN, AND MAY RESULT IN PROBLEMS LATER ON\n# WE WILL ELIMINATE THIS ENTRY PERMANENTLY (INDEX 339)\ndf_train = df_train.drop(index=339).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\">\nENTRY 339 WAS PERMANENTLY DELETED!\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Filling the CABINID field ##","metadata":{}},{"cell_type":"code","source":"# PEARSON CORRELATION BETWEEN CABINID AND OTHER FEATURES\ncorr = pd.get_dummies(df_train.drop(['PassengerId', 'Name', 'Ticket', 'Survived', 'Cabin'], axis=1)).corr()\n# GET ROW AND COLUMN NUMBERS FOR -0.4 < PEARSON CORRELATION < 0.4 (USING ONLY LOWER HALF OF CORRELATION MATRIX)\nrow,col =  np.nonzero(abs(np.triu(corr.iloc[-7:,:-7]))>0.3)\n# GET FEATURES NAME FROM COLUMN INDEXES\ncolname = []\nfor i,j in enumerate(corr.iloc[-8:,:-8].columns):\n    if i in col:\n        colname.append(j)\nprint(colname)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE FUNCTION TO READ FEATURES WITH HIGHEST CORRELATION WITH CABINID (FARE AND FARE_Q4, THE OTHERS ARE REDUNDANT),\n# FOR EACH CABINID=NaN ENTRY, EXTRACT ALL CABINID VALUES FOR SIMILAR FEATURES, \n# AND ASSIGN A RANDOMLY SAMPLED CABINRS VALUE\ndef sampler(fare:float, fare_q:str, cabinid:str):\n    if pd.isnull(cabinid) == True:\n        df = df_train[df_train['CabinId'].isna() == False]\n        cbn_list = list(df[(df['Fare'] == fare) & (df['Fare_Q'] == fare_q)]['CabinId'])\n        if len(cbn_list) != 0:\n            return np.random.choice(cbn_list)\n        else:\n            # MOST COMMON CABINID VALUE IS C\n            return 'C'\n    else:\n        return cabinid\n# RUN FUNCTION OVER CABINID\ndf_train['CabinRS'] = pd.Categorical(df_train.apply(lambda x: sampler(x['Fare'], x['Fare_Q'], x['CabinId']), axis=1), categories = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'O'])\n# FOR TEST DATA, USING THE SAME FUNCTION AS FOR THE TRAIN DATA\ndf_test['CabinRS'] = pd.Categorical(df_test.apply(lambda x: sampler(x['Fare'], x['Fare_Q'], x['CabinId']), axis=1), categories = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'O'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Note on CABINID:</b>\n\n* There are no features having a Pearson correlation coefficients larger than 0.4 with CABINID;\n    * This was likely the cause of the failure in modeling CABINID with regressions, trees, and Naive Bayes (another notebook)\n* The random sampling strategy will be tested, but likely abandoned;\n* CABINID values will be imputed as the mode value of all entries.\n</div>","metadata":{}},{"cell_type":"code","source":"# FINDING MOST COMMON CABINID VALUES\nprint(df_train['CabinId'].value_counts())\n# FILL NaN VALUES IN CABINID WITH CABINID=C\ndf_train['CabinC'] = df_train['CabinId'].fillna('C')\ndf_test['CabinC'] = df_test['CabinId'].fillna('C')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLOT CABINID FEATURES\nfig_cbn = plt.figure(figsize = (14,7))\ngs = plt.GridSpec(2,3, height_ratios=(1,1))\n# SET GRID POSITIONS FOR PLOTS\ng1 = plt.subplot(gs[0, 0])\ng2 = plt.subplot(gs[0, 1])\ng3 = plt.subplot(gs[0, 2])\ng4 = plt.subplot(gs[1, 0])\ng5 = plt.subplot(gs[1, 1])\n# PLOT CABINID, CABINRS, CABINC\nax_list = ['ax1', 'ax2', 'ax3']\ngpts_list = [g1, g2, g3]\nfeat_list = ['CabinId', 'CabinRS', 'CabinC']\nxlbl_list = ['Cabin (original)', 'Cabin (fill=random)', 'Cabin (fill=mode)']\nylbl_list = ['P(Cabin & Survived)', '', '']\nfor axis, gp, feat, xlabel, ylabel in zip(ax_list, gpts_list, feat_list, xlbl_list, ylbl_list):\n    axis = sns.histplot(ax=gp, data = df_train, x=feat,hue = 'Survived', multiple='dodge', stat='probability')\n    axis.set_xlabel(xlabel, fontsize=12)\n    axis.set_ylabel(ylabel, fontsize=12)\n    axis.set_ylim(0,0.6)\n# PLOT COMPARISON OF ORIGINAL DISTRIBUTION AND DISTRIBUTIONS WITH ASSIGNED VALUES\nax4 = sns.histplot(ax=g4, data=df_train, x='CabinId', stat='probability', color='tab:green', label='Original')\nax4 = sns.histplot(ax=g4, data=df_train, x='CabinRS', stat='probability', color='tab:brown', label='Sampled', alpha=0.4)\nax4.legend()\nax4.set_xlabel('Cabin', fontsize=12)\nax4.set_ylabel('P(Cabin)', fontsize=12)\nax4.set_ylim(0,0.9)\nax5 = sns.histplot(ax=g5, data=df_train, x='CabinId', stat='probability', color='tab:green', label='Original')\nax5 = sns.histplot(ax=g5, data=df_train, x='CabinC', stat='probability', color='tab:red', label='Mode=C', alpha=0.4)\nax5.legend()\nax5.set_xlabel('Cabin', fontsize=12)\nax5.set_ylabel('P(Cabin)', fontsize=12)\nax5.set_ylim(0,0.9)\nfig_cbn.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ASSIGNING NaN VALUES IN CABIN ID TO THE CATEGORY O(THER)\ndf_train['CabinIdO'] = pd.Categorical(df_train['CabinId'], categories = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'O']).fillna('O')\ndf_test['CabinIdO'] = pd.Categorical(df_test['CabinId'], categories = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'O']).fillna('O')\n# PLOT CABINID FEATURES\nfig_cbno = plt.figure(figsize = (12,3))\n# PLOT CABINID, CABINIDO\ng1 = plt.subplot(1,2,1)\nsns.histplot(ax=g1, data=df_train, x='CabinId', stat='probability', color='tab:green', label='Original')\nsns.histplot(ax=g1, data=df_train, x='CabinIdO', stat='probability', color='tab:brown', label='NaN=O(ther)', alpha=0.4)\ng1.legend()\ng1.set_xlabel('Cabin', fontsize=12)\ng1.set_ylabel('P(Cabin)', fontsize=12)\ng1.set_ylim(0,0.9)\ng2 = plt.subplot(1,2,2)\nsns.histplot(ax=g2, data=df_train, x='CabinIdO', stat='probability', hue='Survived', multiple='dodge')\ng2.set_xlabel('Cabin', fontsize=12)\ng2.set_ylabel('P(Cabin)', fontsize=12)\ng2.set_ylim(0,0.9)\nfig_cbno.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATING BINARY FEATURE FOR CABINS WITH LARGER ODDS OF SURVIVING\ndef cabsurv(cbn):\n    if cbn in ['B', 'C', 'D', 'E', 'F']:\n        return 1\n    else:\n        return 0\ndf_train['cabinsurvival'] = df_train['CabinIdO'].apply(cabsurv)\ndf_test['cabinsurvival'] = df_test['CabinIdO'].apply(cabsurv)\n# APPEND FEATURE\nselect_features.append('cabinsurvival')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>Note on CABINID AND CABINIDO:</b>\n\n* Both random sampling and mode replacement assignment strategies decrease the original distributions' spread, heavily emphasising the occurrence of label C (as expected) while also reversing the probability ratio of surviving/not_surviving for cabin C;\n    * this means the most populated CABINID feature now reports the opposite trend in survival probability;\n* Missing CABINID entries will be instead assigned the value O(ther); this at least preserves the original distributions and has a large not_survive/survive ratio.\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Filling the AGE field ##","metadata":{}},{"cell_type":"code","source":"# PEARSON CORRELATION BETWEEN CABINID AND OTHER FEATURES\ncorr = pd.get_dummies(df_train.drop(['PassengerId', 'Name', 'Ticket', 'Survived', 'Cabin', 'CabinId', 'CabinRS', 'CabinC'], axis=1)).corr()\n# SORTED CORRELATION VALUES\nprint('Ten highest correlated features with age:\\n', pd.concat([corr.iloc[1,:].sort_values().head(5), corr.iloc[1,:].sort_values().tail(5)]))\n# KEEP TITLE, PCLASS, SIBSP, FAMILY\n# FOR EACH AGE=NaN ENTRY, EXTRACT ALL AGE VALUES FOR SIMILAR FEATURES, AND ASSIGN A RANDOMLY SAMPLED AGE VALUE\ndef sampler(pclass:int, family:str, sibsp:str, title:str, age:int):\n    if pd.isnull(age) == True:\n        df = df_train[df_train['Age'].isna() == False]\n        age_list = list(df[(df['Pclass'] == pclass) & (df['Family'] == family) & (df['SibSp'] == sibsp) & (df['Title'] == title)]['Age'])\n        if len(age_list) != 0:\n            return np.random.choice(age_list)\n        else:\n            # MOST COMMON AGE VALUE IS 22.0\n            return 22.0\n    else:\n        return age\n# RUN FUNCTION OVER AGE\ndf_train['Age_RS'] = df_train.apply(lambda x: sampler(x['Pclass'], x['Family'], x['SibSp'], x['Title'], x['Age']), axis=1)\n# FOR TEST DATA, USING THE SAME FUNCTION AS FOR THE TRAIN DATA\ndf_test['Age_RS'] = df_test.apply(lambda x: sampler(x['Pclass'], x['Family'], x['SibSp'], x['Title'], x['Age']), axis=1)\n\n# FILL NaN VALUES IN AGE WITH MEDIAN AGE VALUE\nagemed = df_train['Age'].quantile(0.5)\ndf_train['Agemed'] = df_train['Age'].fillna(agemed)\ndf_test['Agemed'] = df_test['Age'].fillna(agemed)\n# BINNING AGE_RS INTO PERCENTILES\nq1 = df_train['Age_RS'].quantile(0.25)\nq2 = df_train['Age_RS'].quantile(0.50)\nq3 = df_train['Age_RS'].quantile(0.75)\n\ndef ageq(age):\n    if age<q1:\n        return 1\n    elif age<q2:\n        return 2\n    elif age<q3:\n        return 3\n    else:\n        return 4\n\n# IMPUTE AGE QUANTILES\ndf_train['Age_Q'] = df_train['Age_RS'].apply(ageq)\ndf_test['Age_Q'] = df_test['Age_RS'].apply(ageq)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLOT AGE FEATURES\nfig_age = plt.figure(figsize = (14,7))\ngs = plt.GridSpec(2,3)\n# SET GRID POSITIONS FOR PLOTS\ng1 = plt.subplot(gs[0, 0])\ng2 = plt.subplot(gs[0, 1])\ng3 = plt.subplot(gs[0, 2])\ng4 = plt.subplot(gs[1, 0])\ng5 = plt.subplot(gs[1, 1])\ng6 = plt.subplot(gs[1, 2])\n# PLOT AGE, AGE_RS, AGEMED, AGE_Q\nax_list = ['ax1', 'ax2', 'ax3', 'ax6']\ngpts_list = [g1, g2, g3, g6]\nfeat_list = ['Age', 'Age_RS', 'Agemed', 'Age_Q']\nxlbl_list = ['Age (Original)', 'Age (Sampled)', 'Age (Median=22)', 'Age Percentiles']\nylbl_list = ['P(Cabin & Survived)', '', '', '']\nfor axis, gp, feat, xlabel, ylabel in zip(ax_list, gpts_list, feat_list, xlbl_list, ylbl_list):\n    axis = sns.histplot(ax=gp, data = df_train, x=feat,hue = 'Survived', multiple='dodge', stat='probability')\n    axis.set_xlabel(xlabel, fontsize=12)\n    axis.set_ylabel(ylabel, fontsize=12)\n# PLOT COMPARISON OF ORIGINAL DISTRIBUTION AND DISTRIBUTIONS WITH ASSIGNED VALUES\nax4 = sns.histplot(ax=g4, data=df_train, x='Age', stat='probability', color='tab:green', label='Original', binwidth=0.6)\nax4 = sns.histplot(ax=g4, data=df_train, x='Age_RS', stat='probability', color='tab:brown', label='Sampled', binwidth=0.6, alpha=0.4)\nax4.legend()\nax4.set_xlabel('Age', fontsize=12)\nax4.set_ylabel('P(Age)', fontsize=12)\nax5 = sns.histplot(ax=g5, data=df_train, x='Age', stat='probability', color='tab:green', label='Original', binwidth=0.6)\nax5 = sns.histplot(ax=g5, data=df_train, x='Agemed', stat='probability', color='tab:red', label='Median=22', binwidth=0.6, alpha=0.4)\nax5.legend()\nax5.set_xlabel('Age', fontsize=12)\nax5.set_ylabel('', fontsize=12)\nfig_age.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE THE CHILD FEATURE ACCORDING TO AGE DISTRIBUTION FOR SURVIVED=YES\ndef child(age):\n    if age<=5:\n        return 'yes'\n    else:\n        return 'no'\ndf_train['Child'] = df_train['Age_RS'].apply(child)\ndf_test['Child'] = df_test['Age_RS'].apply(child)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLOT AGE FEATURES\nfig_age = plt.figure(figsize = (14,3))\ngs = plt.GridSpec(1,3)\n# SET GRID POSITIONS FOR PLOTS\ng1 = plt.subplot(gs[0, 0:2])\ng2 = plt.subplot(gs[0, 2])\n# PLOT AGERS\nax1 = sns.histplot(ax=g1, data=df_train, x='Age_RS', stat='probability', hue='Survived', multiple='dodge')\nax1.set_xlabel('Age', fontsize=12)\nax1.set_ylabel('', fontsize=12)\nax2 = sns.histplot(ax=g2, data=df_train, x='Child', stat='probability', hue='Survived', multiple='dodge')\nax2.set_xlabel('Child', fontsize=12)\nax2.set_ylabel('', fontsize=12)\nfig_age.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# APPEND FEATURE\nselect_features.append('Age_Q'), select_features.append('Child')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>AGE_RS feature:</b>\n\n* AGE_RS seems to mimick the original AGE distribution quite well;\n* Children under 5 have a higher probability to survive.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>About lack of AGE and CABIN records:</b>\n\n* AGE/CABIN records, both missing and existing, follow SEX, PCLASS, EMBARK, PARCH+SIBSP records\n    * this is related to the large proportion of SEX=male, PCLASS=3, EMBARK=S, and PARCH+SIBSP=0\n* there are sufficient AGE data that missing AGE records can be estimated\n    * regression model\n    * random sampling from distribution\n    * NaiveBayes\n    * ultimately, modeling didn't work well - the predictions were poor, so we went with random sampling instead\n* for CABIN data, there are many more missing data points\n    * all CABIN data start with one or more alphabet characters, from which CABINID was built\n    * CABINID varies the most with PCLASS\n    * CABINID could be estimated\n        * also, modeling didn't work well here\n        * no type of assignment worked - we're dropping CABIN information\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Bivariate distributions","metadata":{}},{"cell_type":"code","source":"# SURVIVABILITY, FARE_Q AND FAMSIZE\nfig_ = plt.figure(figsize = (12,3))\ng = sns.catplot(data =df_train[['Famsize', 'Fare_Q', 'Survived', 'PassengerId']].groupby(['Famsize', 'Fare_Q', 'Survived']).count().reset_index(),\n            x='Fare_Q', y='PassengerId', hue= 'Survived', col='Famsize', kind='bar')\nfor ax in g.axes.flatten():\n    ax.set_xlabel('Fare_Q', fontsize=12)\n    ax.set_ylabel('Number of Passengers', fontsize=12)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NUMBER OF SURVIVED=1 OR SURVIVED=0 PASSENGERS BY FAMSIZE AND FARE_Q\ndf_live=pd.DataFrame(df_train.loc[df_train['Survived']==1, ['Famsize', 'Fare_Q', 'Survived']].groupby(['Famsize', 'Fare_Q']).count()).reset_index()\ndf_die=pd.DataFrame(df_train.loc[df_train['Survived']==0, ['Famsize', 'Fare_Q', 'Survived']].groupby(['Famsize', 'Fare_Q']).count()).reset_index()\n# CALCULATE RATIO OF SURVIVED TO DEAD\ndf_LD = df_live.merge(df_die, on=['Famsize', 'Fare_Q'], how='left')\ndef ratio_ld(live,die):\n    if die != 0:\n        return (live/die)\n    else:\n        return 0    \ndf_LD['Ratio_LD'] = df_LD.apply(lambda x: ratio_ld(x['Survived_x'], x['Survived_y']), axis=1)\n# ASSIGN THE CALCULATED RATIO TO COMBINATIONS OF FAMSIZE & FARE_Q\ndef famfare(fam, fare):\n        return df_LD.loc[(df_LD['Famsize']==fam) & (df_LD['Fare_Q']==fare), 'Ratio_LD'].iloc[0]\ndf_train['famfare'] = df_train.apply(lambda x: famfare(x['Famsize'], x['Fare_Q']), axis=1)\ndf_test['famfare'] = df_test.apply(lambda x: famfare(x['Famsize'], x['Fare_Q']), axis=1)\ndel df_LD, df_live, df_die\nsns.histplot(data=df_train, x='famfare', hue='Survived', stat='probability', multiple='dodge')\n# APPEND FEATURE\nselect_features.append('famfare')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SURVIVABILITY, SEX AND PCLASS\nfig_ = plt.figure(figsize = (12,3))\ng = sns.catplot(data =df_train[['Sex', 'Pclass', 'Survived', 'PassengerId']].groupby(['Sex', 'Pclass', 'Survived']).count().reset_index(),\n            x='Pclass', y='PassengerId', hue= 'Survived', col='Sex', kind='bar')\nfor ax in g.axes.flatten():\n    ax.set_xlabel('Pclass', fontsize=12)\n    ax.set_ylabel('Number of Passengers', fontsize=12)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NUMBER OF SURVIVED=1 OR SURVIVED=0 PASSENGERS BY SEX AND PCLASS\ndf_live=pd.DataFrame((df_train.loc[df_train['Survived']==1, ['Sex', 'Pclass', 'Survived']].groupby(['Sex', 'Pclass']).count())).reset_index()\ndf_die=pd.DataFrame((df_train.loc[df_train['Survived']==0, ['Sex', 'Pclass', 'Survived']].groupby(['Sex', 'Pclass']).count())).reset_index()\n# CALCULATE RATIO OF SURVIVED TO DEAD\ndf_LD = df_live.merge(df_die, on=['Sex', 'Pclass'], how='left')\ndef ratio_ld(live,die):\n    if die != 0:\n        return (live/die)//10\n    else:\n        return 0    \ndf_LD['Ratio_LD'] = df_LD.apply(lambda x: ratio_ld(x['Survived_x'], x['Survived_y']), axis=1)\n# ASSIGN THE CALCULATED RATIO TO COMBINATIONS OF FAMSIZE & FARE_Q\ndef sexclass(sex, pclass):\n        return df_LD.loc[(df_LD['Sex']==sex) & (df_LD['Pclass']==pclass), 'Ratio_LD'].iloc[0]\ndf_train['sexclass'] = df_train.apply(lambda x: sexclass(x['Sex'], x['Pclass']), axis=1)\ndf_test['sexclass'] = df_test.apply(lambda x: sexclass(x['Sex'], x['Pclass']), axis=1)\ndel df_LD, df_live, df_die\nsns.histplot(data=df_train, x='sexclass', hue='Survived', stat='probability', multiple='dodge')\n# APPEND FEATURE\nselect_features.append('sexclass')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(select_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling #","metadata":{"execution":{"iopub.status.busy":"2023-10-23T09:06:18.016823Z","iopub.execute_input":"2023-10-23T09:06:18.017448Z","iopub.status.idle":"2023-10-23T09:06:18.025049Z","shell.execute_reply.started":"2023-10-23T09:06:18.017405Z","shell.execute_reply":"2023-10-23T09:06:18.023160Z"}}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Modeling strategy:</b>\n    \n* We will calibrate a series of models, to be used collectively for the final prediction;\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Set-up variables, convert and scale data ##","metadata":{}},{"cell_type":"code","source":"print(select_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE DATAFRAMES WITH SELECTED FEATURES\ndf_train_select=df_train[select_features].copy()\ndf_test_select=df_test[select_features].copy()\n# SELECT FEATURES FROM EDA\ndf_train_select = df_train_select.drop(['Sex', 'Pclass'],axis=1)\ndf_test_select = df_test_select.drop(['Sex', 'Pclass'],axis=1)\n# CONVERT CATEGORICAL DATA FOR LOGISTIC REGRESSION CLASSIFIER WITH ONE-HOT ENCODING\n# DEFINE PREDICTOR AND OUTCOME VARIABLES\nXdum = pd.get_dummies(df_train_select, drop_first=False)\nXdum_test = pd.get_dummies(df_test_select, drop_first=False)\ny = df_train['Survived']\n\n# SELECT TOP 20 FEATURES BASED ON CHI^2 TEST\nfeat_index = SelectKBest(chi2, k=20).fit(Xdum,y).get_support(indices=True)\nXncd = Xdum.iloc[:,feat_index]\nXncd_test = Xdum_test.iloc[:,feat_index]\n\n# PLOT PEARSON CORRELATION BETWEEN FEATURES\ncorr = Xncd.corr()\nuppertri = np.triu(corr)\nfig = plt.figure(figsize=(12,8))\nax1=sns.heatmap(corr, annot=True, cmap='Reds', mask=uppertri)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GET ROW AND COLUMN NUMBERS FOR LARGE PEARSON CORRELATION (BEYOND +/-0.75)\nrow,col =  np.nonzero((abs(np.tril(corr))>0.75))\n# REMOVE REPEATED ENTRIES, E.G. 1,1 AND 2,2\nlrow = [int(i) if i!=j else '' for i,j in zip(row,col)]\n# REMOVE COLUMNS FROM Xncd\ncolname = []\nfor i,j in enumerate(corr.columns):\n    if i in lrow:\n        colname.append(j)\nXncd = Xncd.drop(colname, axis=1)\nXncd_test = Xncd_test.drop(colname, axis=1)        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SPLIT DATA INTO TRAINING AND TRIAL\nX_train, X_trial, y_train, y_trial = train_test_split(Xncd, y, test_size=0.25, stratify=y, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FUNCTION TO RUN AND PLOT RESULTS OF MODEL_PREDICT\ndef model_preds(model_name, model_type, data_trial=X_trial, data_test=Xncd_test):\n    \"\"\"\n    Given a model_name (e.g. model_lg, model_rf, etc) and type (logreg, tree, vector, nb), \n    it will return the model predictions (e.g. preds_lg, preds_lg_test) from the \n    trial and test data (X_trial, and Xncd_test as defaults), a classification report,\n    a confusion matrix, and a feature importance/coefficients/excess odds plot for the model.\n    \"\"\"\n    # CHECK FOR CORRECT MODEL_TYPE\n    if model_type not in ['logreg', 'tree', 'vector', 'nb']:\n        print('ERROR: model_type must be set as either tree, logreg, vector or nb.')\n        return\n    # MAKE PREDICTIONS ON TRIAL AND TEST DATA\n    preds = model_name.predict(data_trial)\n    preds_test = model_name.predict(data_test)    \n    # CLASSIFICATION REPORT\n    labels = ['Predicted not to survive', 'Predicted to survive']\n    clfrep = classification_report(y_trial, preds, target_names=labels)\n    print(clfrep)\n    # PLOT CONFUSION MATRIX\n    fig_cm = plt.figure(figsize=(2,2))\n    cm = confusion_matrix(y_trial, preds, labels = model_name.classes_)\n    disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=model_name.classes_)\n    disp.plot(values_format='')\n    fig_cm.show()\n    # PLOT FEATURE IMPORTANCE, ODDS, COEFFICIENTS\n    if model_type == 'tree':\n        # FEATURE IMPORTANCE TREE-BASED MODELS\n        fig_fi = plt.figure(figsize=(12,3))\n        importance = pd.DataFrame()\n        importance['Feature'] = data_trial.columns\n        importance['Importance'] = model_name.named_steps['model_tree'].feature_importances_\n        importance = importance.sort_values(by='Importance', ascending=False)\n        ax = sns.barplot(data=importance, x='Feature', y='Importance')\n        ax.set_xlabel('Feature', fontsize=16)\n        ax.set_ylabel('Feature Importance', fontsize=16)\n        plt.xticks(rotation = 45)\n        fig_fi.show()\n    elif model_type == 'logreg':\n        # ODDS FROM LOGISTIC REGRESSION\n        fig_odd = plt.figure(figsize = (12,3))\n        odds = pd.DataFrame()\n        odds['Feature'] = data_trial.columns\n        odds['Odds'] = (np.exp(model_name.named_steps['model_lg'].coef_.flatten()))\n        odds = odds.sort_values(by = 'Odds', ascending=False, key=abs)\n        ax=sns.barplot(data=odds, x='Feature', y='Odds')\n        ax.set_xlabel('Feature', fontsize=16)\n        ax.set_ylabel('Odds (Survival)', fontsize=16)\n        plt.xticks(rotation = 45)        \n        fig_odd.show()\n    elif model_type == 'vector':\n        # VECTOR MAGNITUDES\n        fig_vec = plt.figure(figsize = (12,3))\n        mags = pd.DataFrame()\n        mags['Feature'] = data_trial.columns\n        mags['Components'] = np.exp(model_name.named_steps['model_svc'].coef_.flatten())\n        mags = mags.sort_values(by = 'Components', ascending=False)\n        ax=sns.barplot(data = mags, x='Feature', y='Components')\n        ax.set_xlabel('Feature', fontsize=16)\n        ax.set_ylabel('Vector Components', fontsize=16)\n        plt.xticks(rotation = 45)\n        fig_vec.show()\n    # RETURN PREDICTIONS\n    return preds, preds_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fit model: Logistic Regression ##","metadata":{}},{"cell_type":"code","source":"%%time\n# SETUP MODEL, CROSS-VALIDATION\n# PIPELINE FOR CV - THIS IS SCALING THE DATASET FOR EVERY CV FOLD, \n# EVEN THE ONES USING THE SAME DATA FOR TRAINING/TESTING, WHICH IS VEEERY INNEFICIENT...\npl_lg = Pipeline([('scale', StandardScaler()),\n                ('model_lg', LogisticRegressionCV(max_iter=1000, tol=0.0001, random_state=0))])\n# CV PARAMETERS\npar_grid_lg = {'model_lg__Cs' : [50, 100, 200, 250]}\n#CROSS-VALIDATION\n# FIT MODEL\nmodel_grid_lg = GridSearchCV(pl_lg, param_grid=par_grid_lg, cv=5, refit='f1', n_jobs = -1, verbose = 1)\nmodel_grid_lg.fit(X_train, y_train)\n# BEST PREDICTOR\nbest_lg = model_grid_lg.best_estimator_\nprint('Best CV parameters: ', model_grid_lg.best_params_)\n# GET PREDICTIONS\npreds_lg, preds_test_lg = model_preds(best_lg, 'logreg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fit model: Random Forest ##","metadata":{}},{"cell_type":"raw","source":"%%time\n# SETUP MODEL, CROSS-VALIDATION\n# PIPELINE FOR CV - THIS IS SCALING THE DATASET FOR EVERY CV FOLD, \n# EVEN THE ONES USING THE SAME DATA FOR TRAINING/TESTING, WHICH IS VEEERY INNEFICIENT...\n# ALSO, SCALING ISN'T NECESSARY FOR TREES, BUT WE'RE BUILDING AN ENSEMBLE MODEL AT THE END,\n# SO ALL THE DATA SHOULD BE EITHER SCALED OR NOT\npl_rf = Pipeline([('scale', StandardScaler()),\n                ('model_tree', RandomForestClassifier(criterion='entropy', random_state=0))])\n# CV PARAMETERS\npar_grid_rf = {'model_tree__max_depth' : [10,12,15], \n             'model_tree__max_features' : [0.5,0.7,0.9], \n             'model_tree__max_samples' : [0.7,0.8,0.9], \n             'model_tree__min_samples_leaf' : [0.001,0.005,0.01], \n             'model_tree__min_samples_split' : [0.001,0.005,0.01], \n             'model_tree__n_estimators' : [150, 200, 250]}\n#CROSS-VALIDATION\n# FIT MODEL\nmodel_grid_rf = GridSearchCV(pl_rf, param_grid=par_grid_rf, cv=5, refit='f1', n_jobs = -1, verbose = 1)\nmodel_grid_rf.fit(X_train, y_train)\n# BEST PREDICTOR\nbest_rf = model_grid_rf.best_estimator_\nprint('Best CV parameters: ', model_grid_rf.best_params_)\n# GET PREDICTIONS\npreds_rf, preds_test_rf = model_preds(best_rf, 'tree')","metadata":{"execution":{"iopub.status.busy":"2023-11-07T14:48:33.245934Z","iopub.execute_input":"2023-11-07T14:48:33.246340Z","iopub.status.idle":"2023-11-07T14:59:36.488979Z","shell.execute_reply.started":"2023-11-07T14:48:33.246308Z","shell.execute_reply":"2023-11-07T14:59:36.488013Z"}}},{"cell_type":"markdown","source":"## Fit model: XGB ##","metadata":{}},{"cell_type":"code","source":"%%time\n# SETUP MODEL, CROSS-VALIDATION\n# PIPELINE FOR CV - THIS IS SCALING THE DATASET FOR EVERY CV FOLD, \n# EVEN THE ONES USING THE SAME DATA FOR TRAINING/TESTING, WHICH IS VEEERY INNEFICIENT...\n# ALSO, SCALING ISN'T NECESSARY FOR TREES, BUT WE'RE BUILDING AN ENSEMBLE MODEL AT THE END,\n# SO ALL THE DATA SHOULD BE EITHER SCALED OR NOT\npl_xgb = Pipeline([('scale', StandardScaler()),\n                ('model_tree', XGBClassifier(objective = 'binary:logistic', random_state=0))])\n# CV PARAMETERS\npar_grid_xgb = {'model_tree__max_depth' : [3, 4, 5],\n                'model_tree__n_estimators' : [150, 200, 250],\n                'model_tree__learning_rate' : [0.1, 0.2, 0.3],\n                'model_tree__colsample_bytree' : [0.7, 0.8, 0.9],\n                'model_tree__min_child_weight': [7, 10, 13],\n                'model_tree__gamma' : [0.7, 0.8, 0.9],\n                'model_tree__subsample' : [0.5, 0.7, 0.9]}\n#CROSS-VALIDATION\n# FIT MODEL\nmodel_grid_xgb = GridSearchCV(pl_xgb, param_grid=par_grid_xgb, cv=5, refit='f1', n_jobs = -1, verbose = 1)\nmodel_grid_xgb.fit(X_train, y_train)\n# BEST PREDICTOR\nbest_xgb = model_grid_xgb.best_estimator_\nprint('Best CV parameters: ', model_grid_xgb.best_params_)\n# GET PREDICTIONS\npreds_xgb, preds_test_xgb = model_preds(best_xgb, 'tree')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fit model: (Linear) Support Vector Classifier ##","metadata":{}},{"cell_type":"raw","source":"%%time\n# SETUP MODEL, CROSS-VALIDATION\n# PIPELINE FOR CV - THIS IS SCALING THE DATASET FOR EVERY CV FOLD, \n# EVEN THE ONES USING THE SAME DATA FOR TRAINING/TESTING, WHICH IS VEEERY INNEFICIENT...\npl_svc = Pipeline([('scale', StandardScaler()),\n                ('model_svc', LinearSVC(fit_intercept=True, random_state=0))])\n# CV PARAMETERS\npar_grid_svc = {'model_svc__dual' : [False],\n                'model_svc__tol' : [0.0001, 0.000001],\n                'model_svc__C' : [0.005, 0.01, 0.1, 0.5, 1.0, 1.5, 2.0],\n                'model_svc__class_weight' : [None, 'balanced'],\n                'model_svc__max_iter' : [1000, 10000, 100000, 1000000]}\n#CROSS-VALIDATION\n# FIT MODEL\nmodel_grid_svc = GridSearchCV(pl_svc, param_grid=par_grid_svc, cv=5, refit='f1', n_jobs = -1, verbose = 1)\nmodel_grid_svc.fit(X_train, y_train)\n# BEST PREDICTOR\nbest_svc = model_grid_svc.best_estimator_\nprint('Best CV parameters: ', model_grid_svc.best_params_)\n# GET PREDICTIONS\npreds_svc, preds_test_svc = model_preds(best_svc, 'vector')","metadata":{"execution":{"iopub.status.busy":"2023-11-07T14:38:18.963108Z","iopub.status.idle":"2023-11-07T14:38:18.963536Z","shell.execute_reply.started":"2023-11-07T14:38:18.963338Z","shell.execute_reply":"2023-11-07T14:38:18.963357Z"}}},{"cell_type":"markdown","source":"## Fit model: Bernoulli Naive Bayes Classifier ##","metadata":{}},{"cell_type":"raw","source":"%%time\n# SETUP MODEL, CROSS-VALIDATION\n# PIPELINE FOR CV - THIS IS SCALING THE DATASET FOR EVERY CV FOLD, \n# EVEN THE ONES USING THE SAME DATA FOR TRAINING/TESTING, WHICH IS VEEERY INNEFICIENT...\nmodel_ber = BernoulliNB()\n# CV PARAMETERS\npar_grid_ber = {'alpha' : [0.1, 0.2, 0.3, 0.4, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]}\n\n#CROSS-VALIDATION\n# FIT MODEL\nmodel_grid_ber = GridSearchCV(model_ber, param_grid=par_grid_ber, cv=5, refit='f1', n_jobs = -1, verbose = 1)\nmodel_grid_ber.fit(X_train, y_train)\nprint('Best CV parameters: ', model_grid_ber.best_params_)\n# GET PREDICTIONS\npreds_ber, preds_test_ber = model_preds(model_grid_ber, 'nb')","metadata":{"execution":{"iopub.status.busy":"2023-11-07T14:38:18.965767Z","iopub.status.idle":"2023-11-07T14:38:18.966394Z","shell.execute_reply.started":"2023-11-07T14:38:18.966181Z","shell.execute_reply":"2023-11-07T14:38:18.966203Z"}}},{"cell_type":"markdown","source":"## Fit model: Gaussian Naive Bayes Classifier ##","metadata":{}},{"cell_type":"raw","source":"%%time\n# SETUP MODEL, CROSS-VALIDATION\n# PIPELINE FOR CV - THIS IS SCALING THE DATASET FOR EVERY CV FOLD, \n# EVEN THE ONES USING THE SAME DATA FOR TRAINING/TESTING, WHICH IS VEEERY INNEFICIENT...\nmodel_mn = MultinomialNB()\n# CV PARAMETERS\npar_grid_mn = {'alpha' : [0.1, 0.2, 0.3, 0.4, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]}\n\n#CROSS-VALIDATION\n# FIT MODEL\nmodel_grid_mn = GridSearchCV(model_mn, param_grid=par_grid_mn, cv=5, refit='f1', n_jobs = -1, verbose = 1)\nmodel_grid_mn.fit(X_train, y_train)\nprint('Best CV parameters: ', model_grid_ber.best_params_)\n# GET PREDICTIONS\npreds_mn, preds_test_mn = model_preds(model_grid_mn, 'nb')","metadata":{"execution":{"iopub.status.busy":"2023-11-07T14:38:18.967911Z","iopub.status.idle":"2023-11-07T14:38:18.968794Z","shell.execute_reply.started":"2023-11-07T14:38:18.968488Z","shell.execute_reply":"2023-11-07T14:38:18.968519Z"}}},{"cell_type":"markdown","source":"## Stacking Classifier model ##","metadata":{}},{"cell_type":"raw","source":"# BUILD INPUT FOR STACKING CLASSIFIER\nx_stack=pd.DataFrame({'Logistic_Regression':preds_lg, 'Random_Forest':preds_rf, 'XGBoosting':preds_xgb, 'Linear-SVC':preds_svc, 'Multinomial_NB':preds_mn, 'Bernoulli_MB':preds_ber})\ny_stack = y_trial\nx_stack_test=pd.DataFrame({'Logistic_Regression':preds_test_lg, 'Random_Forest':preds_test_rf, 'XGBoosting':preds_test_xgb, 'Linear-SVC':preds_test_svc, 'Multinomial_NB':preds_test_mn, 'Bernoulli_NB':preds_test_ber})\n# PLOT PEARSON CORRELATION FOR INPUT\ncorr = x_stack.corr()\nuppertri = np.triu(corr)\nfig = plt.figure(figsize=(8,6))\nax1=sns.heatmap(corr, annot=True, cmap='Reds', mask=uppertri)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T14:38:18.972655Z","iopub.status.idle":"2023-11-07T14:38:18.973054Z","shell.execute_reply.started":"2023-11-07T14:38:18.972863Z","shell.execute_reply":"2023-11-07T14:38:18.972881Z"}}},{"cell_type":"raw","source":"# GET RID OF HIGHLY CORRELATED MODELS\n#x_stack = x_stack.drop(['Linear-SVC'], axis=1)\n#x_stack_test = x_stack_test.drop(['Linear-SVC'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T14:38:18.974993Z","iopub.status.idle":"2023-11-07T14:38:18.975369Z","shell.execute_reply.started":"2023-11-07T14:38:18.975187Z","shell.execute_reply":"2023-11-07T14:38:18.975205Z"}}},{"cell_type":"raw","source":"%%time\n# BUILD STACKING CLASSIFIER LOGISTIC REGRESSION MODEL\nmodel_stack = LogisticRegressionCV(Cs = 500, cv = 5, scoring = 'f1', max_iter = 1000, tol=0.0001,refit=True)\n#model_stack = XGBClassifier(colsample_bytree = 0.8, gamma=0.7, learning_rate=0.1, max_depth = 4,\n                                #min_child_weight = 7, n_estimators = 400, subsample = 0.9, random_state = 0)\n\n# FIT THE MODEL\nmodel_stack.fit(x_stack, y_stack)\n# USE MODEL TO MAKE PREDICTIONS ON TRIAL DATASET\npreds_stack = model_stack.predict(x_stack_test)\n\n# CREATE OUTPUT FILE FOR SUBMISSION\noutput = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': preds_stack})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T14:38:18.977192Z","iopub.status.idle":"2023-11-07T14:38:18.977593Z","shell.execute_reply.started":"2023-11-07T14:38:18.977381Z","shell.execute_reply":"2023-11-07T14:38:18.977398Z"}}}]}